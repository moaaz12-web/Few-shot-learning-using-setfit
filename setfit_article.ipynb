{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALL SETFIT"
      ],
      "metadata": {
        "id": "PA2Ah-jDaim4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEmnBYjlYBWP"
      },
      "outputs": [],
      "source": [
        "! pip install setfit[optuna]==0.3.0 datasets -U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset\n",
        "We are going to use the ag_news dataset, which a news article classification dataset with 4 classes: World (0), Sports (1), Business (2), Sci/Tech (3).\n",
        "\n",
        "The test split of the dataset contains 7600 examples, which is will be used to evaluate our model. The train split contains 120000 examples, which is a nice amount of data for fine-tuning a regular model.\n",
        "\n",
        "But to shwocase SetFit, we wanto to create a dataset with only a 8 labeled samples per class, or 32 data points.\n",
        "\n"
      ],
      "metadata": {
        "id": "XggPvFrnaCcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "XwPYHUkneA1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "aLgATGNdcvnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset,concatenate_datasets\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "# create train dataset\n",
        "seed=20\n",
        "labels = 4\n",
        "samples_per_label = 8\n",
        "sampled_datasets = []\n",
        "# find the number of samples per label\n",
        "for i in range(labels):\n",
        "    sampled_datasets.append(dataset[\"train\"].filter(lambda x: x[\"label\"] == i).shuffle(seed=seed).select(range(samples_per_label)))\n",
        "\n",
        "# concatenate the sampled datasets\n",
        "train_dataset = concatenate_datasets(sampled_datasets)\n",
        "\n",
        "# create test dataset\n",
        "test_dataset = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "XfK3NWYJYQdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PY5TpOc9aLv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tune Classifier with SetFit\n",
        "When using SetFit we first fine-tune a Sentence Transformer model using our labeled data and contrastive training, where positive and negative pairs are created by in-class and out-class selection. The second step a classification head is trained on the encoded embeddings with their respective class labels.\n",
        "\n",
        "As Sentence Transformers we are going to use sentence-transformers/all-mpnet-base-v2. (you could replace the model with any available sentence transformer on hf.co).\n",
        "\n",
        "The Python SetFit package is implementing useful classes and functions to make the fine-tuning process straightforward and easy. Similar to the Hugging Face Trainer class, SetFits implmenets the SetFitTrainer class is responsible for the training loop."
      ],
      "metadata": {
        "id": "C7cQowX3aMU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from setfit import SetFitModel, SetFitTrainer\n",
        "from sentence_transformers.losses import CosineSimilarityLoss\n",
        "\n",
        "# Load a SetFit model from Hub\n",
        "model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model = SetFitModel.from_pretrained(model_id)\n",
        "\n",
        "# Create trainer\n",
        "trainer = SetFitTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    loss_class=CosineSimilarityLoss,\n",
        "    metric=\"accuracy\",\n",
        "    batch_size=64,\n",
        "    num_iterations=20, # The number of text pairs to generate for contrastive learning\n",
        "    num_epochs=1, # The number of epochs to use for constrastive learning\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()\n",
        "\n",
        "print(f\"model used: {model_id}\")\n",
        "print(f\"train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"accuracy: {metrics['accuracy']}\")\n",
        "\n",
        "#    model used: sentence-transformers/all-mpnet-base-v2\n",
        "#    train dataset: 32 samples\n",
        "#    accuracy: 0.8647368421052631"
      ],
      "metadata": {
        "id": "7QRnqhDhYRpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Hyperparameter search to optimize result\n",
        "The SetFitTrainer provides a hyperparameter_search() method that you can use to find the perefect hyperparameters for the data. SetFit is leveraging optuna under the hood to perform the hyperparameter search. To use the hyperparameter search, we need to define a model_init method, which creates our model for every \"run\" and a hp_space method that defines the hyperparameter search space.\n",
        "\n"
      ],
      "metadata": {
        "id": "iJklfuuHaS5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from setfit import SetFitModel, SetFitTrainer\n",
        "from sentence_transformers.losses import CosineSimilarityLoss\n",
        "\n",
        "# model specfic hyperparameters\n",
        "def model_init(params):\n",
        "    params = params or {}\n",
        "    max_iter = params.get(\"max_iter\", 100)\n",
        "    solver = params.get(\"solver\", \"liblinear\")\n",
        "    model_id = params.get(\"model_id\", \"sentence-transformers/all-mpnet-base-v2\")\n",
        "    model_params = {\n",
        "        \"head_params\": {\n",
        "            \"max_iter\": max_iter,\n",
        "            \"solver\": solver,\n",
        "        }\n",
        "    }\n",
        "    return SetFitModel.from_pretrained(model_id, **model_params)\n",
        "\n",
        "# training hyperparameters\n",
        "def hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
        "        \"num_epochs\": trial.suggest_int(\"num_epochs\", 1, 5),\n",
        "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32]),\n",
        "        \"num_iterations\": trial.suggest_categorical(\"num_iterations\", [5, 10, 20, 40, 80]),\n",
        "        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
        "        \"max_iter\": trial.suggest_int(\"max_iter\", 50, 300),\n",
        "        \"solver\": trial.suggest_categorical(\"solver\", [\"newton-cg\", \"lbfgs\", \"liblinear\"]),\n",
        "        \"model_id\": trial.suggest_categorical(\n",
        "            \"model_id\",\n",
        "            [\n",
        "                \"sentence-transformers/all-mpnet-base-v2\",\n",
        "                \"sentence-transformers/all-MiniLM-L12-v1\",\n",
        "            ],\n",
        "        ),\n",
        "    }\n",
        "\n",
        "\n",
        "trainer = SetFitTrainer(\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    model_init=model_init,\n",
        ")\n",
        "\n",
        "best_run = trainer.hyperparameter_search(direction=\"maximize\", hp_space=hp_space, n_trials=100)\n"
      ],
      "metadata": {
        "id": "7yywBPFQYXYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After, we have found the perfect hyperparameters we need to run a last training using those."
      ],
      "metadata": {
        "id": "tk0sYpWXaand"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.apply_hyperparameters(best_run.hyperparameters, final_model=True)\n",
        "trainer.train()\n",
        "\n",
        "metrics = trainer.evaluate()\n",
        "\n",
        "print(f\"model used: {best_run.hyperparameters['model_id']}\")\n",
        "print(f\"train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"accuracy: {metrics['accuracy']}\")\n",
        "\n",
        "# model used: sentence-transformers/all-mpnet-base-v2\n",
        "# train dataset: 32 samples\n",
        "# accuracy: 0.873421052631579"
      ],
      "metadata": {
        "id": "77zWF_SPaa7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EOgVyRUsac4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "Thats it, we have created a high-performing text-classification model with only 32 labeled samples or 8 samples per class using the SetFit approach. Our SetFit classifier achieved an accuracy of 0.873421052631579 on the test set. For comparison a regular model fine-tuned on the whole dataset (12 000) achieves a performance ~94% accuracy."
      ],
      "metadata": {
        "id": "T_vMNkwdaeuG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AsPCbUr5afh6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}